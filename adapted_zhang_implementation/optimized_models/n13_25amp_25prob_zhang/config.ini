[optuna_optimization]
ntrials = 64
optuna_metric = average_val_fidelity

[optimization_system_parameters]

[optimization_learning_parameters]
gamma = [0.95, 1.0, False, 'float']
fc1_dims = [512, 2048, False, 'int']
learning_rate = [1e-05, 0.01, True, 'float']

[experiment]
new_experiment = True
experiment_alias = opt_for_n13_25amp_25prob_optuna_parallel_fixed/trial_36
experiment_description = optimization of fc1_dims, lr and gamma value for original actions and reward in chain of size 13 noise (0,25, =25)
directory_name = opt_for_n13_25amp_25prob_optuna_parallel_fixed

[system_parameters]
chain_length = 13
tstep_length = 0.15
tolerance = 0.05
max_t_steps = 65
field_strength = 100
coupling = 1
n_actions = 16
action_set = zhang

[learning_parameters]
prioritized_experience_replay = True
number_of_features = 26
number_of_episodes = 20000
learning_rate = 0.0005507103376242569
gamma = 0.9843005450750567
replace_target_iter = 200
memory_size = 40000
batch_size = 32
epsilon = 0.99
epsilon_increment = 0.0001
fc1_dims = 1213
fc2_dims = 404
dropout = 0.0
reward_function = original

[noise_parameters]
noise = True
noise_probability = 0.25
noise_amplitude = 0.25

[tags]
reward_function = original
prioritized = prioritized
mlflow.note.content = optimization of fc1_dims, lr and gamma value for original actions and reward in chain of size 13 noise (0,25, =25)
action set = zhang
chain_length = 13
noise = yes
optuna_optimization = True

